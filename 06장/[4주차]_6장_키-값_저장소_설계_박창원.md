# 6장 - 키-값 저장소 설계

# 키-값 저장소란?

- 키-값 저장소(key-value store)는 비 관계형 데이터베이스다.
- 고유 식별자(identifier)를 키로 갖는다.
- 키와 값 사이의 연결관계를 “키-값" 쌍(pair)이라고 한다.
- 키의 사례는 “last_logged_int_at” 처럼 텍스트 키와 “253DDEC4” 처럼 해시 키가 있다.
- 키-값 저장소의 대표적 사례는 아마존 다이나모, memcached, redis 가 있다.

키의 길이는 어느정도가 적절할까? 

성능상 이유로 키는 짧을 수록 좋다.

# 키-값 저장소 설계 요구사항

다음의 특성을 갖는 키-값 저장소를 설계해보자.

- 키-값 쌍의 크기는 10KB 이하
- 큰 데이터 저장 가능
- 높은 가용성(장애 시에도 빠르게 응답)
- 높은 규모 확장성(트래픽 양에 따라 자동 서버 증설 및 삭제 가능)
- 데이터 일관성 수준 조절 가능
- 응답 지연시간(latency) 최소

# 단일 키-값 저장소

먼저 한 대의 서버만 사용하는 키-값 저장소를 설계해보자.

메모리에 해시 테이블로 키-값 쌍을 모두 저장하는 것은 어떨까?

이 방법은 빠르지만, 모든 데이터를 메모리에 저장하는 것은 현실적으로 불가능하다.

이 문제점을 해결하는 방법은 다음과 같다.

- 데이터 압축
- 데이터를 메모리와 디스크에 분할하여 저장

단일 키-값 저장소는 한계가 있으므로, 분산 저장소를 고려해야 한다. 

# 분산 키-값 저장소

분산 시스템 설계시에는 CAP 이론을 알아야 한다.

## CAP 정리(이론)

CAP 정리의 정의는 다음과 같다.

- 일관성(consistency)
    - 클라이언트는 어떤 노드에 접속 했느냐에 관계 없이 항상 같은 데이터를 읽어야 한다.
- 가용성(availability)
    - 클라이언트는 일부 노드에 장애가 발생해도 항상 응답을 받아야 한다.
- 파티션 감내성(partition tolerance)
    - 두 노드 사이 통신 장애가 발생해도, 시스템은 계속 동작해야 한다.

애석하게도, CAP의 3가지 요소는 모두 충족될 수 없다.

CAP의 두 가지를 만족하는 경우의 수는 다음과 같다.

- CP시스템
    - 일관성과 파티션 감내를 지원
- AP 시스템
    - 가용성과 파티션 감내를 지원
- CA 시스템(존재하지 않음)
    - 일관성과 가용성을 지원
    - 분산시스템은 반드시 파티션 문제를 감내할 수 있어야 한다.
    - 실세계에 CA 시스템은 존재하지 않는다.

장애 상황 예시를 통해 다시 이해해보자.

## 이상적 상태

- 첫번째 노드(n1)와 n2, n3가 서로 연결되어 있다.
- 데이터 일관성과 가용성 모두 만족한다.

## 실세계 분산 시스템

- 불시에 파티션 문제가 발생한다.
- 만약 n3에 장애가 발생한다면, 클라이언트가 n1, n2에 기록한 데이터가 n3에 전달되지 않는다.
- 따라서, 일관성을 만족하지 못한다.

만약 이 상황에서 일관성을 선택한다면?

- n1과 n2 에 쓰기 연산을 중단한다.
- 가용성을 만족하지 못한다.
- 은행권 시스템은 일관성이 중요하므로 문제 해결 전까지 클라이언트에 오류를 반환한다.

# 키-값 저장소 구현을 위한 핵심 컴포넌트

- 데이터 파티션(partition)
- 데이터 다중화(replication)
- 일관성(consistency)
- 일관성 불일치 해소(inconsistency resolution)
- 장애 처리
- 시스템 아키텍처 다이어그램
- 쓰기 경로(write path)
- 읽기 경로(read path)

# 데이터 파티션

데이터를 파티션 단위로 나눌때 고려해야할 요소는 무엇일까?

- 데이터를 여러 서버에 고르게 분산할 수 있는가
- 노드를 추가 하거나 삭제할 때 데이터의 이동을 최소화할 수 있는가?

안정 해시(consistent hash)는 이런 문제를 푸는 적합한 기술이다.

# 데이터 다중화

높은 가용성을 위해 다중화는 필수다.

데이터를 N개 서버에 비동기적으로 다중화하려고 할 때, N개의 서버를 어떻게 선택할까?

키를 해시 링 위에 배치하여 그 지점으로부터 시계 방향으로 링을 순회하여 만나는 첫 N개 서버에 데이터 사본을 보관한다.

아래 예제는 N=3 으로 설정하여 key0이 s1, s2, s3 에 저장된다.

```plaintext
 s0 (key0)-> s1 -> s2 -> s3 --> ...
```

# 데이터 일관성

클라이언트가 어느 노드에 쓰기 작업을 할 때, 어덯게 다른 노드에 반영이 되는 걸까?

이러한 동기화를 위해 정족수 합의(Quorum Consensus) 프로토콜이 필요하다.

이 프로토콜은 3가지 값이 필요하다.

- 쓰기가 성공했다고 인정하기 위해, 몇 개의 노드에 쓰기 연산이 완료되었다고 응답받아야 할까?
    - 이것을 **쓰기 연산에 대한 정족수(W)**라고 한다.
- 읽기가 성공했다고 인정하기 위해, 몇 개의 노드에 읽기 연산이 완료되었다고 응답받아야 할까?
    - 이것을 **읽기 연산에 대한 정족수(R)**라고 한다.
- 사본의 수(N)

N, W, R의 값을 어떻게 정할까? 이것은 상황에 따라 다르며, 다음과 같이 분류할 수 있다.

- R=1, W=N: 빠르게 읽을 수 있다.
- W=1, R=N: 빠르게 쓸 수 있다.
- W+R>N 강한 일관성이 보장된다.

# 일관성 모델

비즈니스의 특성과 요구사항에 따라서 일관성의 정도를 결정해야 한다.

일관성 수준의 정도는 다음과 같다.

- 강한 일관성(strong consistency)
    - 모든 읽기는 가장 최근에 갱신된 결과를 반환한다.
- 약한 일관성(weak consistency)
    - 모든 읽기가 가장 최근에 갱싱된 결과를 반환하지 못 할 수도 있다.
- 최종 일관성(eventual consistency)
    - 약한 일관성 중 하나의 형태다
    - 갱신 결과가 결국에는 모든 사본에 동기화(반영)된다.

**주의할 점은, 강한 일관성 모델은 고가용성 시스템에는 적합하지 않다는 것이다. 왜 일까?**

모든 사본에 현재 쓰기 연산의 결과가 반영되기 전까지 이 데이터에 대해서는 읽기와 쓰기를 할 수 없도록 막아야하기 때문이다.

어떤 DB가 최종 일관성 모델일까?

- 다이나모와 카산드라가 있다.

# 비 일관성 해소기법

## 데이터 버저닝(versioning)

데이터를 다중화할 때 피할 수 없는 문제는?

- 가용성은 보장되지만 사본 간 일관성을 지키기가 어렵다는 점이다.
- 이를 해결하기 위한 것이 버저닝이다.

버저닝이란?

- 데이터를 변경할 때마다 해당 데이터의 새로운 버전을 만드는 것
- 각 버전 데이터는 불변(immutable)이다.

두 클라이언트가 동시에 같은 키 값으로 두 개의 사본 저장소에 쓰기 작업을 한다면 어떤 문제가 발생할까?

- 키에 대해 값이 2가지로 충돌된다.

이를 해결하기 위한 것이 벡터 시계(vector clock)이다.

벡터 시계는 어떻게 동작할까?

- 벡터 시계의 표현법
    - Data ( [서버1, 버전1], [서버2, 버전2], ... [서버n, 버전n] )
- 만약, 데이터를 서버1에 등록 한다면, 다음 작업 중 하나를 수행한다.
    - [서버1, 버전1]가 있으면 버전1을 하나 증가시킨다.
    - 없다면, 새 항목 [서버1, 버전1]을 생성한다.

다음은 벡터 시계의 사용 시나리오다.

- 서버1이 쓰기 연산 → D1[서버1, 버전1]
- 서버1이 쓰기 연산 → D1[서버1, 버전2]
- 서버2와 서버3이 동시에 쓰기 연산
    - D3([서버1, 버전2], [서버2, 버전1])
    - D3([서버1, 버전2], [서버3, 버전1])
- 서버1이 읽기를 하려고 하는데 충돌이 발생함, 서버 1은 충돌 해소위해 새로 기록
    - D5([서버1, 버전 3], [서버2, 버전1], [서버3, 버전1])

벡터 시계의 단점은 무엇일까?

- 충돌 감지 및 해소로직이 클라이언트가 수행해야 한다.
- [서버: 버전] 순서쌍의 개수가 빠르게 증가한다.

# 장애 처리 - 장애 감지

만약 한 대 서버가 죽었다면 어떤 방법을 통해 장애로 간주해야할까?

- 멀티캐스팅(multicasting)
    - 각각의 서버 서로를 모두 연결한다.
    - 서버가 많아질 수록 비효율적이다.
- 가십 프로토콜(gossip protocol)
    - 멤버십 목록(membership list)를 저장한다.
    - 멤버십 목록에는 각 멤버(서버)ID와 해당 멤버의 박동 카운터(heartbeat counter)를 갖는다.
    - 각 노드는 주기적으로 자신의 박동 카운터를 증가한다.
    - 각 노드는 무작위로 선정하여 주기적으로 자기 박동 카운터 값을 보낸다.
    - 박동 카운터 값을 받은 노드는 멤버십 목록을 최신으로 갱신한다.
    - **어떤 멤버의 박동 카운터 값이 지정된 시간 동안 갱신되지 않을 경우, 해당 멤버를 장애 상태로 간주한다.**

# 장애 처리

장애를 감지하였으면 다음 할일은 무엇인가?

장애를 처리해야한다. 왜냐하면, 클라이언트에게 끊김없이 응답을 유지하기 위해서다.

네트워크나 서버 문제일 경우 단순히 다른 서버가 잠시 맡아 처리한다.

장애 서버가 복구될 때 변경사항을 일괄 반영하면 된다.

영구적인 장애를 처리해야할 경우는 어떻게 해야할까?

대표적으로 반-엔트로피(anti-entropy) 프로토콜을 구현한다.

반-엔트로피 프로토콜이란?

- 사본들을 비교하여 최신 버전으로 갱신하는 방법

동시에 이를 효율적으로 처리해야한다.

즉, 사본 간 일관성이 망가진 상태를 탐지하고 전송 데이터양을 줄여야한다.

이 경우 머클(Merkle) 트리를 사용한다.

머클 트리란?

- 해시 트리라고도 불린다.
- 각 노드에 그 자식 노드들에 보관된 값의 해시(자식 노드가 leaf 노드일 경우)
- 또는, 자식 노드들의 레이블로부터 계산된 해시 값을 레이블로 붙여두는 트리

예를 들어 설명하자. 

키 공간이 1부터 12까지 구성될 경우,

1. 키 공간을 버킷으로 나눈다.
2. 버킷에 포함된 각각의 키에 균등 분포 해시 함수를 이용하여 해시 값을 계산한다.
3. 버킷별로 해시값을 계산한다. 해당 해시 값을 레이블로 갖는 노드를 생성한다.
4. 자식 노드의 레이블로부터 새로운 해시 값을 계산하여 이진 트리를 상향식으로 만들어 나간다.

# 데이터 센터 장애 처리

데이터 센터 장애를 대응하려면 여러 데이터 센터에 다중화하는 것이 일반적이다.

# 시스템 아키텍처 다이어그램

키-값 저장소의 컴포넌트를 살펴보았으니 이 요소들을 합쳐보자.

- 클라이언트는 API( get(key), put(key, value) ) 와 통신한다.
- 중재자는 클라이언트에게 키-값 저장소에 대해 proxy 역할을 한다.
- 노드는 안정 해시의 해시 링 위에 분포된다.
- 노드를 자동으로 추가 삭제할 수 있도록 시스템은 분산된다.
- 데이터는 여러 노드에 다중화 된다.
- SPOF(Single Point of Failure)는 존재하지 않는다.(모든 노드가 같은 책임을 지므로)

# 카산드라의 쓰기 및 읽기 과정 예제

다음은 카산드라에 쓰기 요청을 했을 때 처리 순서다.

1. 쓰기 요청을 커밋 로그 파일에 기록한다.
2. 데이터를 메모리 캐시에 기록한다.
3. 메모리 캐시가 가득찬다면?
4. SSTable(Sorted-String Table)로 구현된 디스크에 저장한다.

(여기서, SSTable 이란 <키,값> 순서쌍을 정렬된 리스트 형태로 관리하는 테이블임)

다음은 카산드라에 읽기 요청을 했을 때 처리 순서다.

1. 가장 먼저 데이터가 메모리 캐시에 있는지 살핀다.
2. 데이터가 메모리에 없다면? 다른 노드가 가지고 있다는 의미다.
3. 블룸필터(Bloom Filter)를 검사한다.
4. 블룸필터는 어떤 SSTable에 키가 보관됐는지 검사한다.
5. SSTable에서 데이터를 가져와 클라이언트에게 반환한다.

# 요약

분산 키-값 저장소의 문제들과 해당 문제에 대한 기술들을 정리해보자.

| 목표/문제 | 기술 |
| --- | --- |
| 대규모 데이터 저장 | 안정 해시를 사용하여 서버에 부하 분산 |
| 읽기 연산에 대한 가용성 보장 | 여러 데이터센터에 다중화 |
| 쓰기 연산에 대한 가용성 보장 | 버저닝 및 벡터 시계를 사용하여 충돌 해소 |
| 데이터 파티션 | 안정 해시 |
| 점진적 규모 확장성 | 안정 해시 |
| 다양성(heterogeneity) | 안정 해시 |
| 조절 가능한 데이터 일관성 | 정족수 합의(quorum consensus) |
| 일시적 장애 처리 | 느슨한 정족수 프로토콜 및 단서 후 임시 위탁 |
| 영구적 장애 처리 | 머클 트리 |
| 데이터 센터 장애 대응 | 여러 데이터센터에 다중화 |
